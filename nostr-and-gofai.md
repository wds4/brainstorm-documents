Can Nostr Save Good Old Fashioned AI?
=====

Many people may not be aware that the current crop of AI taking the world by storm known as connectionist AI -- LLMs, neural networks, machine learning, etc -- is not the only game in town when it comes to AI. There is an entirely separate category of AI called symbolic AI, rules based AI, or what we will call Good Old Fashioned AI: GOFAI. Indeed, GOFAI was the dominant school of thought for many decades. And there are still adherents, such as Josh Tenenbaum, who consider GOFAI to be the real deal. But in recent decades, progress in GOFAI has been eclipsed by progress in connectionist AI. So what went wront with GOFAI?

## The 40-year Knowledge-Acquisition Bottleneck

For four decades, symbolic AI has been haunted by a single, unsolved problem: how to acquire, at scale and without ruinous cost, the structured commonsense and causal knowledge that humans effortlessly trade in conversation.

Douglas Lenat began Cyc in 1984 with the conviction that a sufficiently large, hand-crafted ontology of explicit axioms—encoded by professional ontologists—would eventually let machines reason like bright ten-year-olds. After forty years and several hundred million dollars, Cyc contains roughly 25 million axioms yet still fails at simple tasks that any child (or any two-day-fine-tuned LLM) solves instantly: understanding that a glass will spill if turned upside-down, that “my enemy’s enemy is my friend,” or that promising to come tomorrow usually implies intent to come tomorrow. The reason is not inference; it is acquisition. Central teams of paid experts cannot anticipate the infinite contexts, cultures, and edge cases the world throws at a reasoning system. Cyc proved that the bottleneck is not logical but social and economic.

The same bottleneck re-appeared, in almost identical form, in the most sophisticated modern descendant of symbolic AI: Josh Tenenbaum’s program of hierarchical Bayesian program induction (Lake et al., 2015; Ellis et al., 2021). Systems such as Bayesian Program Learning and DreamCoder can, in carefully engineered micro-domains, invent rich symbolic programs—new letters, physics engines, list-processing abstractions—from one or a handful of examples. They do this by performing Bayesian inference over a space of short, compositional programs biased by strong inductive priors (simplicity, reuse, core knowledge). The models are psychologically faithful and computationally elegant. Yet they remain confined to toy worlds because no one has ever supplied them with the vast, messy, culturally transmitted library of reusable abstractions that human children absorb from language and observation. Without that library, the prior is either too weak (explosion of hypotheses) or too strong (hand-engineered by the researcher, defeating the entire point).

Large language models appear to have sidestepped the problem entirely by memorizing statistical correlations across trillions of tokens. They can parrot commonsense remarkably well, but they do not possess executable causal models, cannot reliably chain reasoning beyond a few steps, and hallucinate confidently when the statistical pattern runs out. In Tenenbaum’s terms, they are masters of System 1 intuition without System 2 understanding. Billions of dollars continue to pour into scaling this approach, while the symbolic research program that once promised genuine understanding receives essentially zero industry funding.

The core claim of this paper is that the bottleneck is no longer technical. The missing piece is not a better inference algorithm or a cleverer prior; it is a writable, cryptographically signed, globally replicated substrate on which millions of humans can continuously propose, critique, refine, and weight symbolic structures. The nostr protocol, combined with the mechanisms described below, provides exactly that substrate for the first time. Brainstorm is the client that turns it into a personal, continuously updating symbolic mind.

The rest of the paper shows how.
